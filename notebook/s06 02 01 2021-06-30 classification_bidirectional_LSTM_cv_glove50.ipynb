{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#packages\" data-toc-modified-id=\"packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>packages</a></span></li><li><span><a href=\"#function\" data-toc-modified-id=\"function-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>function</a></span></li><li><span><a href=\"#get-clean-data\" data-toc-modified-id=\"get-clean-data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>get clean data</a></span><ul class=\"toc-item\"><li><span><a href=\"#get-raw-data\" data-toc-modified-id=\"get-raw-data-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>get raw data</a></span></li><li><span><a href=\"#get-doc_list\" data-toc-modified-id=\"get-doc_list-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>get <code>doc_list</code></a></span></li><li><span><a href=\"#get-bi_weapon_array\" data-toc-modified-id=\"get-bi_weapon_array-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>get <code>bi_weapon_array</code></a></span></li><li><span><a href=\"#get-raw-embedding-dictionary\" data-toc-modified-id=\"get-raw-embedding-dictionary-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>get raw embedding dictionary</a></span><ul class=\"toc-item\"><li><span><a href=\"#test\" data-toc-modified-id=\"test-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>test</a></span></li></ul></li></ul></li><li><span><a href=\"#cv\" data-toc-modified-id=\"cv-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>cv</a></span><ul class=\"toc-item\"><li><span><a href=\"#main\" data-toc-modified-id=\"main-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>main</a></span></li></ul></li><li><span><a href=\"#test\" data-toc-modified-id=\"test-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>test</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pre-setting\n",
    "# automatically adjust the width of the notebook code cell\n",
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "# if one module is changed, this line will automatically reload that module\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# display the figure in the notebook\n",
    "%matplotlib inline\n",
    "# To change the font size in acrobat\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add path\n",
    "import os\n",
    "import sys\n",
    "src_dir = os.path.abspath(os.path.join(os.pardir, 'src'))\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.insert(0, src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import gensim as gs\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import label_ranking_average_precision_score, label_ranking_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, SpatialDropout1D, LSTM, Ｂidirectional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.evaluation import get_label_via_training, doc_class_evaluation_fscore, baseline_doc_class_evaluation_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_raw_doc_array_train_test(doc_array_train, doc_array_test, doc_len_qtl, raw_embedding_dic, maxlen=100, max_words=10000):\n",
    "    \n",
    "    # tokenize raw doc based on training\n",
    "    \n",
    "    # # train tokenizer\n",
    "    tokenizer_train = Tokenizer(num_words=max_words)\n",
    "    tokenizer_train.fit_on_texts(doc_array_train)\n",
    "\n",
    "    # # get dictionary of {word: wordID}\n",
    "    dic_word_wordID_train = tokenizer_train.word_index\n",
    "    num_unique_word_in_train = len(dic_word_wordID_train.keys())\n",
    "\n",
    "    # # transfer the word in raw doc to wordID\n",
    "    sequences_train = tokenizer_train.texts_to_sequences(doc_array_train)\n",
    "    sequences_test = tokenizer_train.texts_to_sequences(doc_array_test)\n",
    "\n",
    "    # # make all doc have the same length\n",
    "    data_array_train = pad_sequences(sequences_train, maxlen=maxlen)\n",
    "    data_array_test = pad_sequences(sequences_test, maxlen=maxlen)\n",
    "    \n",
    "    \n",
    "    # create embedding_matrix for dnn\n",
    "    \n",
    "    # # get word embedding dimension from the raw_embedding_dic\n",
    "    # # raw_embedding_dic is from pre-trained dataset from other researchers\n",
    "    embedding_dim = len(list(raw_embedding_dic.values())[0])\n",
    "\n",
    "    # # get embedding_matrix from raw_embedding_dic for later proposed dnn\n",
    "    embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "    for word, i in dic_word_wordID_train.items():\n",
    "        embedding_vector = raw_embedding_dic.get(word)\n",
    "        if i < max_words:\n",
    "            if embedding_vector is not None:\n",
    "                # Words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                \n",
    "    return data_array_train, data_array_test, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_dnn_model_predict(data_train, label_train, data_test, label_test, max_words, embedding_dim, maxlen, embedding_matrix, epochs=10):\n",
    "    \n",
    "    # end layer node\n",
    "    end_layer_node_num = label_train.shape[1]\n",
    "    \n",
    "    # build the dnn model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "    model.add(Ｂidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(end_layer_node_num, activation='sigmoid'))\n",
    "    \n",
    "    # set word embedding matrix\n",
    "    model.layers[0].set_weights([embedding_matrix])\n",
    "    model.layers[0].trainable = False\n",
    "    \n",
    "    # model compile\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='mse',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    # training the model\n",
    "    model.fit(data_train, label_train, epochs=epochs, batch_size=32)\n",
    "    \n",
    "    # predict on testing\n",
    "    \n",
    "    prob_predict_test = model.predict(data_test)\n",
    "    \n",
    "    label_test_predict = get_label_via_training(prob_predict_test, label_train)\n",
    "    \n",
    "    tmp_f1_score_dic = doc_class_evaluation_fscore(label_test_predict, label_test)\n",
    "\n",
    "    return tmp_f1_score_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_6p2_folder = os.path.abspath(os.path.join(os.pardir, 'data', 's2021_lumen_clean_data'))\n",
    "email_6p2_file = 's2021_06_20_01_lumen_clean_doc_sia_liwc_classify.csv'\n",
    "email_6p2_location = os.path.join(email_6p2_folder, email_6p2_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_6p2_df = pd.read_csv(email_6p2_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_6p2_df = email_6p2_df.rename(columns={'scarcity_time':'scarcity'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2771, 35)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text_id</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>text_type</th>\n",
       "      <th>nostop_stem_doc</th>\n",
       "      <th>nostop_stem_doc_len</th>\n",
       "      <th>clean_doc</th>\n",
       "      <th>clean_doc_len</th>\n",
       "      <th>pos_sia</th>\n",
       "      <th>compound_sia</th>\n",
       "      <th>neu_sia</th>\n",
       "      <th>neg_sia</th>\n",
       "      <th>posemo_liwc</th>\n",
       "      <th>negemo_liwc</th>\n",
       "      <th>anx_liwc</th>\n",
       "      <th>anger_liwc</th>\n",
       "      <th>sad_liwc</th>\n",
       "      <th>reward_liwc</th>\n",
       "      <th>risk_liwc</th>\n",
       "      <th>time_liwc</th>\n",
       "      <th>money_liwc</th>\n",
       "      <th>Authority or Expertise/Source Credibility</th>\n",
       "      <th>Blame/guilt</th>\n",
       "      <th>Commitment</th>\n",
       "      <th>Commitment- Call to Action</th>\n",
       "      <th>Commitment- Indignation</th>\n",
       "      <th>Emphasis</th>\n",
       "      <th>Gain framing</th>\n",
       "      <th>Liking</th>\n",
       "      <th>Loss framing</th>\n",
       "      <th>Objectivity</th>\n",
       "      <th>Reciprocation</th>\n",
       "      <th>Scarcity/Urgency/Opportunity</th>\n",
       "      <th>Social Proof</th>\n",
       "      <th>Social Proof- Admonition</th>\n",
       "      <th>Subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"A Baker Swept By,\" by Edward Hirsch Audio: Re...</td>\n",
       "      <td>news left</td>\n",
       "      <td>baker swept edward hirsch audio read author al...</td>\n",
       "      <td>69</td>\n",
       "      <td>a baker swept by by edward hirsch audio read b...</td>\n",
       "      <td>137</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.7506</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.020</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Get Out\" Won’t Have A 100% Rating On Rotten T...</td>\n",
       "      <td>news left</td>\n",
       "      <td>get rate rotten tomato ever fact jordan peel g...</td>\n",
       "      <td>99</td>\n",
       "      <td>get out won t have a rating on rotten tomatoes...</td>\n",
       "      <td>194</td>\n",
       "      <td>0.115</td>\n",
       "      <td>-0.5607</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.113</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>\"Know Your Rights or Your Safety Is At Risk In...</td>\n",
       "      <td>russian ad</td>\n",
       "      <td>know right safeti risk interact polic shock vi...</td>\n",
       "      <td>76</td>\n",
       "      <td>know your rights or your safety is at risk in ...</td>\n",
       "      <td>146</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-0.8751</td>\n",
       "      <td>0.790</td>\n",
       "      <td>0.142</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Nancy Pelosi was drunk again today,\" begins a...</td>\n",
       "      <td>fake news</td>\n",
       "      <td>nanci pelosi drunk today begin post recent sha...</td>\n",
       "      <td>90</td>\n",
       "      <td>nancy pelosi was drunk again today begins a po...</td>\n",
       "      <td>172</td>\n",
       "      <td>0.078</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.092</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>\"Obama out\": POTUS ends speech with viral mic ...</td>\n",
       "      <td>news left</td>\n",
       "      <td>obama potu end speech viral mic drop presid ba...</td>\n",
       "      <td>58</td>\n",
       "      <td>obama out potus ends speech with viral mic dro...</td>\n",
       "      <td>105</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.040</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   raw_text_id                                           raw_text   text_type  \\\n",
       "0            0  \"A Baker Swept By,\" by Edward Hirsch Audio: Re...   news left   \n",
       "1            1  \"Get Out\" Won’t Have A 100% Rating On Rotten T...   news left   \n",
       "2            3  \"Know Your Rights or Your Safety Is At Risk In...  russian ad   \n",
       "3            4  \"Nancy Pelosi was drunk again today,\" begins a...   fake news   \n",
       "4            5  \"Obama out\": POTUS ends speech with viral mic ...   news left   \n",
       "\n",
       "                                     nostop_stem_doc  nostop_stem_doc_len  \\\n",
       "0  baker swept edward hirsch audio read author al...                   69   \n",
       "1  get rate rotten tomato ever fact jordan peel g...                   99   \n",
       "2  know right safeti risk interact polic shock vi...                   76   \n",
       "3  nanci pelosi drunk today begin post recent sha...                   90   \n",
       "4  obama potu end speech viral mic drop presid ba...                   58   \n",
       "\n",
       "                                           clean_doc  clean_doc_len  pos_sia  \\\n",
       "0  a baker swept by by edward hirsch audio read b...            137    0.075   \n",
       "1  get out won t have a rating on rotten tomatoes...            194    0.115   \n",
       "2  know your rights or your safety is at risk in ...            146    0.068   \n",
       "3  nancy pelosi was drunk again today begins a po...            172    0.078   \n",
       "4  obama out potus ends speech with viral mic dro...            105    0.047   \n",
       "\n",
       "   compound_sia  neu_sia  neg_sia  posemo_liwc  negemo_liwc  anx_liwc  \\\n",
       "0        0.7506    0.905    0.020            4            1         0   \n",
       "1       -0.5607    0.772    0.113            6            6         0   \n",
       "2       -0.8751    0.790    0.142            4            2         1   \n",
       "3       -0.1027    0.831    0.092            6            1         0   \n",
       "4        0.1779    0.913    0.040            2            1         0   \n",
       "\n",
       "   anger_liwc  sad_liwc  reward_liwc  risk_liwc  time_liwc  money_liwc  \\\n",
       "0           0         1            0          1         14           0   \n",
       "1           0         0            7          1         10           0   \n",
       "2           0         0            2          4          3           1   \n",
       "3           0         0            5          0         11           1   \n",
       "4           0         0            1          0          6           0   \n",
       "\n",
       "   Authority or Expertise/Source Credibility  Blame/guilt  Commitment  \\\n",
       "0                                          0            0           0   \n",
       "1                                          1            1           1   \n",
       "2                                          0            1           1   \n",
       "3                                          1            0           0   \n",
       "4                                          1            0           1   \n",
       "\n",
       "   Commitment- Call to Action  Commitment- Indignation  Emphasis  \\\n",
       "0                           0                        0         0   \n",
       "1                           0                        0         0   \n",
       "2                           1                        1         1   \n",
       "3                           0                        0         0   \n",
       "4                           0                        0         0   \n",
       "\n",
       "   Gain framing  Liking  Loss framing  Objectivity  Reciprocation  \\\n",
       "0             0       0             0            0              0   \n",
       "1             0       1             0            1              0   \n",
       "2             0       0             0            1              0   \n",
       "3             0       0             0            1              0   \n",
       "4             0       1             0            1              0   \n",
       "\n",
       "   Scarcity/Urgency/Opportunity  Social Proof  Social Proof- Admonition  \\\n",
       "0                             1             0                         0   \n",
       "1                             0             0                         0   \n",
       "2                             0             1                         1   \n",
       "3                             0             1                         0   \n",
       "4                             0             1                         0   \n",
       "\n",
       "   Subjectivity  \n",
       "0             1  \n",
       "1             1  \n",
       "2             1  \n",
       "3             1  \n",
       "4             1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(email_6p2_df.shape)\n",
    "email_6p2_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get `doc_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [i.split() for i in email_6p2_df.nostop_stem_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2771"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get',\n",
       " 'rate',\n",
       " 'rotten',\n",
       " 'tomato',\n",
       " 'ever',\n",
       " 'fact',\n",
       " 'jordan',\n",
       " 'peel',\n",
       " 'get',\n",
       " 'one',\n",
       " 'film',\n",
       " 'thriller',\n",
       " 'sit',\n",
       " 'impress',\n",
       " 'fresh',\n",
       " 'rotten',\n",
       " 'tomato',\n",
       " 'one',\n",
       " 'point',\n",
       " 'film',\n",
       " 'held',\n",
       " 'steadi',\n",
       " 'fresh',\n",
       " 'score',\n",
       " 'came',\n",
       " 'halt',\n",
       " 'one',\n",
       " 'review',\n",
       " 'rip',\n",
       " 'project',\n",
       " 'nation',\n",
       " 'review',\n",
       " 'armond',\n",
       " 'white',\n",
       " 'describ',\n",
       " 'get',\n",
       " 'headlin',\n",
       " 'review',\n",
       " 'return',\n",
       " 'movi',\n",
       " 'get',\n",
       " 'actor',\n",
       " 'lakeith',\n",
       " 'stanfield',\n",
       " 'definit',\n",
       " 'hide',\n",
       " 'thought',\n",
       " 'white',\n",
       " 'review',\n",
       " 'twitter',\n",
       " 'movi',\n",
       " 'still',\n",
       " 'sit',\n",
       " 'fresh',\n",
       " 'rotten',\n",
       " 'tomato',\n",
       " 'even',\n",
       " 'though',\n",
       " 'ton',\n",
       " 'glow',\n",
       " 'review',\n",
       " 'sinc',\n",
       " 'come',\n",
       " 'fresh',\n",
       " 'review',\n",
       " 'one',\n",
       " 'rotten',\n",
       " 'one',\n",
       " 'never',\n",
       " 'get',\n",
       " 'back',\n",
       " 'round',\n",
       " 'nearest',\n",
       " 'whole',\n",
       " 'number',\n",
       " 'two',\n",
       " 'except',\n",
       " 'repres',\n",
       " 'rotten',\n",
       " 'tomato',\n",
       " 'told',\n",
       " 'buzzfe',\n",
       " 'news',\n",
       " 'round',\n",
       " 'round',\n",
       " 'reserv',\n",
       " 'absolut',\n",
       " 'movi',\n",
       " 'everi',\n",
       " 'singl',\n",
       " 'review',\n",
       " 'fresh',\n",
       " 'everi',\n",
       " 'singl',\n",
       " 'review',\n",
       " 'rotten',\n",
       " 'armond',\n",
       " 'white',\n",
       " 'say']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66.20064958498737, 66.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_6p2_df.nostop_stem_doc_len.mean(), email_6p2_df.nostop_stem_doc_len.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for dnn (deep neural network) each email should have fixed doc length\n",
    "# here, we can choose 80% quantile\n",
    "doc_len_qtl = int(email_6p2_df.nostop_stem_doc_len.quantile(.80))\n",
    "doc_len_qtl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get `bi_weapon_array`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Authority or Expertise/Source Credibility',\n",
       " 'Commitment',\n",
       " 'Commitment- Call to Action',\n",
       " 'Subjectivity',\n",
       " 'Gain framing',\n",
       " 'Blame/guilt',\n",
       " 'Emphasis']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2021-06-30\n",
    "influence_list = [\n",
    "    'Authority or Expertise/Source Credibility',\n",
    "    'Commitment',\n",
    "    'Commitment- Call to Action',\n",
    "    'Subjectivity',\n",
    "    'Gain framing',\n",
    "    'Blame/guilt',\n",
    "    'Emphasis',\n",
    "]\n",
    "\n",
    "influence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_weapon_array = email_6p2_df[influence_list].values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2771, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 1, 0],\n",
       "       [0, 1, 1, ..., 0, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [1, 1, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(bi_weapon_array.shape)\n",
    "bi_weapon_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_weapon_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get raw embedding dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.03 s, sys: 182 ms, total: 5.21 s\n",
      "Wall time: 5.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "glove_dir = src_dir = os.path.abspath(os.path.join(os.pardir, 'data', 'tmp_glove'))\n",
    "'data/glove'\n",
    "\n",
    "raw_embedding_dic = {}\n",
    "# f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.50d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    raw_embedding_dic[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = len(list(raw_embedding_dic.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(raw_embedding_dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_embedding_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(raw_embedding_dic.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_embedding_dic['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_embedding_dic['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = doc_len_qtl  # We will cut reviews after 100 words\n",
    "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
    "epochs = 10\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f4/kdl4gvs93cdcx85y_wrml1nc0000gn/T/ipykernel_87644/2818858388.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  doc_array = np.array(doc_list)\n"
     ]
    }
   ],
   "source": [
    "doc_array = np.array(doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: 2216 TEST: 555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-06 09:17:39.107872: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-07-06 09:17:39.123381: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd7bf2be280 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-07-06 09:17:39.123399: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2216/2216 [==============================] - 8s 4ms/step - loss: 0.2013 - acc: 0.6435\n",
      "Epoch 2/10\n",
      "2216/2216 [==============================] - 7s 3ms/step - loss: 0.1874 - acc: 0.6408\n",
      "Epoch 3/10\n",
      "2216/2216 [==============================] - 6s 3ms/step - loss: 0.1820 - acc: 0.6327\n",
      "Epoch 4/10\n",
      "2216/2216 [==============================] - 6s 3ms/step - loss: 0.1783 - acc: 0.6236\n",
      "Epoch 5/10\n",
      "2216/2216 [==============================] - 7s 3ms/step - loss: 0.1756 - acc: 0.6232\n",
      "Epoch 6/10\n",
      "2216/2216 [==============================] - 6s 3ms/step - loss: 0.1716 - acc: 0.6241\n",
      "Epoch 7/10\n",
      "2216/2216 [==============================] - 6s 3ms/step - loss: 0.1698 - acc: 0.6151\n",
      "Epoch 8/10\n",
      "2216/2216 [==============================] - 7s 3ms/step - loss: 0.1689 - acc: 0.6218\n",
      "Epoch 9/10\n",
      "2216/2216 [==============================] - 7s 3ms/step - loss: 0.1658 - acc: 0.6313\n",
      "Epoch 10/10\n",
      "2216/2216 [==============================] - 7s 3ms/step - loss: 0.1651 - acc: 0.6101\n",
      "TRAIN: 2217 TEST: 554\n",
      "Epoch 1/10\n",
      "2217/2217 [==============================] - 9s 4ms/step - loss: 0.2009 - acc: 0.6716\n",
      "Epoch 2/10\n",
      "2217/2217 [==============================] - 8s 4ms/step - loss: 0.1864 - acc: 0.6382\n",
      "Epoch 3/10\n",
      "2217/2217 [==============================] - 8s 3ms/step - loss: 0.1812 - acc: 0.6166\n",
      "Epoch 4/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1774 - acc: 0.6220\n",
      "Epoch 5/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1739 - acc: 0.6089\n",
      "Epoch 6/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1719 - acc: 0.6139\n",
      "Epoch 7/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1702 - acc: 0.6166\n",
      "Epoch 8/10\n",
      "2217/2217 [==============================] - 6s 3ms/step - loss: 0.1674 - acc: 0.6184\n",
      "Epoch 9/10\n",
      "2217/2217 [==============================] - 6s 3ms/step - loss: 0.1651 - acc: 0.6103\n",
      "Epoch 10/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1633 - acc: 0.6166\n",
      "TRAIN: 2217 TEST: 554\n",
      "Epoch 1/10\n",
      "2217/2217 [==============================] - 8s 4ms/step - loss: 0.2009 - acc: 0.6455\n",
      "Epoch 2/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1863 - acc: 0.6238\n",
      "Epoch 3/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1812 - acc: 0.6211\n",
      "Epoch 4/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1779 - acc: 0.6252\n",
      "Epoch 5/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1743 - acc: 0.6274\n",
      "Epoch 6/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1716 - acc: 0.6283\n",
      "Epoch 7/10\n",
      "2217/2217 [==============================] - 8s 3ms/step - loss: 0.1709 - acc: 0.6265\n",
      "Epoch 8/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1676 - acc: 0.6189\n",
      "Epoch 9/10\n",
      "2217/2217 [==============================] - 8s 4ms/step - loss: 0.1659 - acc: 0.6288\n",
      "Epoch 10/10\n",
      "2217/2217 [==============================] - 8s 4ms/step - loss: 0.1633 - acc: 0.6157\n",
      "TRAIN: 2217 TEST: 554\n",
      "Epoch 1/10\n",
      "2217/2217 [==============================] - 8s 4ms/step - loss: 0.2013 - acc: 0.6585\n",
      "Epoch 2/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1872 - acc: 0.6279\n",
      "Epoch 3/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1812 - acc: 0.6279\n",
      "Epoch 4/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1782 - acc: 0.6089\n",
      "Epoch 5/10\n",
      "2217/2217 [==============================] - 6s 3ms/step - loss: 0.1752 - acc: 0.6080\n",
      "Epoch 6/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1720 - acc: 0.6062\n",
      "Epoch 7/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1699 - acc: 0.6134\n",
      "Epoch 8/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1677 - acc: 0.6139\n",
      "Epoch 9/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1661 - acc: 0.6130\n",
      "Epoch 10/10\n",
      "2217/2217 [==============================] - 8s 4ms/step - loss: 0.1637 - acc: 0.6107\n",
      "TRAIN: 2217 TEST: 554\n",
      "Epoch 1/10\n",
      "2217/2217 [==============================] - 8s 4ms/step - loss: 0.2005 - acc: 0.6342\n",
      "Epoch 2/10\n",
      "2217/2217 [==============================] - 8s 4ms/step - loss: 0.1867 - acc: 0.6446\n",
      "Epoch 3/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1818 - acc: 0.6392\n",
      "Epoch 4/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1777 - acc: 0.6292\n",
      "Epoch 5/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1758 - acc: 0.6274\n",
      "Epoch 6/10\n",
      "2217/2217 [==============================] - 8s 4ms/step - loss: 0.1733 - acc: 0.6220\n",
      "Epoch 7/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1705 - acc: 0.6261\n",
      "Epoch 8/10\n",
      "2217/2217 [==============================] - 6s 3ms/step - loss: 0.1679 - acc: 0.6211\n",
      "Epoch 9/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1664 - acc: 0.6283\n",
      "Epoch 10/10\n",
      "2217/2217 [==============================] - 7s 3ms/step - loss: 0.1660 - acc: 0.6161\n",
      "CPU times: user 20min 33s, sys: 4min 3s, total: 24min 37s\n",
      "Wall time: 6min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_result_f1_score_save = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in kf.split(doc_array):\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    \n",
    "    # get raw doc train & test\n",
    "    doc_array_train = doc_array[train_index]\n",
    "    doc_array_test = doc_array[test_index]\n",
    "    \n",
    "    # get label train & test\n",
    "    label_train = bi_weapon_array[train_index]\n",
    "    label_test = bi_weapon_array[test_index]\n",
    "\n",
    "    # prepare raw doc train & test for dnn\n",
    "    data_array_train, data_array_test, embedding_matrix = prepare_raw_doc_array_train_test(doc_array_train, doc_array_test, doc_len_qtl, raw_embedding_dic, maxlen=maxlen, max_words=max_words)\n",
    "    \n",
    "    # train & predict & evaluate\n",
    "    tmp_f1_score_dic = fit_dnn_model_predict(data_array_train, label_train, data_array_test, label_test, max_words, embedding_dim, maxlen, embedding_matrix, epochs=epochs)\n",
    "    \n",
    "    # save result\n",
    "    df_result_f1_score_save = df_result_f1_score_save.append(tmp_f1_score_dic, ignore_index=True) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_score</th>\n",
       "      <th>f1_score_macro</th>\n",
       "      <th>f1_score_micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.723552</td>\n",
       "      <td>0.645914</td>\n",
       "      <td>0.696953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.718154</td>\n",
       "      <td>0.629712</td>\n",
       "      <td>0.688870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.729242</td>\n",
       "      <td>0.648410</td>\n",
       "      <td>0.701705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.633222</td>\n",
       "      <td>0.681243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.731821</td>\n",
       "      <td>0.652931</td>\n",
       "      <td>0.705549</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acc_score  f1_score_macro  f1_score_micro\n",
       "0   0.723552        0.645914        0.696953\n",
       "1   0.718154        0.629712        0.688870\n",
       "2   0.729242        0.648410        0.701705\n",
       "3   0.714286        0.633222        0.681243\n",
       "4   0.731821        0.652931        0.705549"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_f1_score_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acc_score         0.723411\n",
       "f1_score_macro    0.642038\n",
       "f1_score_micro    0.694864\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_result_f1_score_save.mean()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "acc_score         0.723411\n",
    "f1_score_macro    0.642038\n",
    "f1_score_micro    0.694864\n",
    "dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "187.891px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
